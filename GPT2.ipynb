{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd23bf7-a9b6-46d3-8881-5e424f2e49db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:47:04.811132Z",
     "iopub.status.busy": "2024-08-22T11:47:04.810839Z",
     "iopub.status.idle": "2024-08-22T11:47:18.487018Z",
     "shell.execute_reply": "2024-08-22T11:47:18.486326Z",
     "shell.execute_reply.started": "2024-08-22T11:47:04.811109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Collecting transformers\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/47/ab/c42556ba7c5aed687256466d472abb9a1b9cbff5730aa42a884d892e061a/transformers-4.44.1-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/b9/8f/d6718641c14d98a5848c6a24d2376028d292074ffade0702940a4b1dde76/huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/18/f3/27bf4d7112b194eea2d8401706953080692d37ace1b74b36fcc7234961cd/safetensors-0.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.5/435.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading https://mirrors.ustc.edu.cn/pypi/packages/48/5d/acf5905c36149bbaec41ccf7f2b68814647347b72075ac0b1fe3022fdc73/tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Installing collected packages: tqdm, safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.24.6 safetensors-0.4.4 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e90c3ab-00f5-4ed8-bb35-5bd88b546e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T11:47:18.488602Z",
     "iopub.status.busy": "2024-08-22T11:47:18.488410Z",
     "iopub.status.idle": "2024-08-22T11:47:20.480714Z",
     "shell.execute_reply": "2024-08-22T11:47:20.480317Z",
     "shell.execute_reply.started": "2024-08-22T11:47:18.488582Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c3e2fc0-a0b1-480a-9b76-32355d6a3e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-22T12:49:28.119095Z",
     "iopub.status.busy": "2024-08-22T12:49:28.118862Z",
     "iopub.status.idle": "2024-08-22T12:49:28.156226Z",
     "shell.execute_reply": "2024-08-22T12:49:28.155911Z",
     "shell.execute_reply.started": "2024-08-22T12:49:28.119082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "wte.weight torch.Size([50257, 768])\n",
      "wpe.weight torch.Size([1024, 768])\n",
      "h.0.ln_1.weight torch.Size([768])\n",
      "h.0.ln_1.bias torch.Size([768])\n",
      "h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.0.attn.c_attn.bias torch.Size([2304])\n",
      "h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.0.attn.c_proj.bias torch.Size([768])\n",
      "h.0.ln_2.weight torch.Size([768])\n",
      "h.0.ln_2.bias torch.Size([768])\n",
      "h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.0.mlp.c_proj.bias torch.Size([768])\n",
      "h.1.ln_1.weight torch.Size([768])\n",
      "h.1.ln_1.bias torch.Size([768])\n",
      "h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.1.attn.c_attn.bias torch.Size([2304])\n",
      "h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.1.attn.c_proj.bias torch.Size([768])\n",
      "h.1.ln_2.weight torch.Size([768])\n",
      "h.1.ln_2.bias torch.Size([768])\n",
      "h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.1.mlp.c_proj.bias torch.Size([768])\n",
      "h.2.ln_1.weight torch.Size([768])\n",
      "h.2.ln_1.bias torch.Size([768])\n",
      "h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.2.attn.c_attn.bias torch.Size([2304])\n",
      "h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.2.attn.c_proj.bias torch.Size([768])\n",
      "h.2.ln_2.weight torch.Size([768])\n",
      "h.2.ln_2.bias torch.Size([768])\n",
      "h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.2.mlp.c_proj.bias torch.Size([768])\n",
      "h.3.ln_1.weight torch.Size([768])\n",
      "h.3.ln_1.bias torch.Size([768])\n",
      "h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.3.attn.c_attn.bias torch.Size([2304])\n",
      "h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.3.attn.c_proj.bias torch.Size([768])\n",
      "h.3.ln_2.weight torch.Size([768])\n",
      "h.3.ln_2.bias torch.Size([768])\n",
      "h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.3.mlp.c_proj.bias torch.Size([768])\n",
      "h.4.ln_1.weight torch.Size([768])\n",
      "h.4.ln_1.bias torch.Size([768])\n",
      "h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.4.attn.c_attn.bias torch.Size([2304])\n",
      "h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.4.attn.c_proj.bias torch.Size([768])\n",
      "h.4.ln_2.weight torch.Size([768])\n",
      "h.4.ln_2.bias torch.Size([768])\n",
      "h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.4.mlp.c_proj.bias torch.Size([768])\n",
      "h.5.ln_1.weight torch.Size([768])\n",
      "h.5.ln_1.bias torch.Size([768])\n",
      "h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.5.attn.c_attn.bias torch.Size([2304])\n",
      "h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.5.attn.c_proj.bias torch.Size([768])\n",
      "h.5.ln_2.weight torch.Size([768])\n",
      "h.5.ln_2.bias torch.Size([768])\n",
      "h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.5.mlp.c_proj.bias torch.Size([768])\n",
      "h.6.ln_1.weight torch.Size([768])\n",
      "h.6.ln_1.bias torch.Size([768])\n",
      "h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.6.attn.c_attn.bias torch.Size([2304])\n",
      "h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.6.attn.c_proj.bias torch.Size([768])\n",
      "h.6.ln_2.weight torch.Size([768])\n",
      "h.6.ln_2.bias torch.Size([768])\n",
      "h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.6.mlp.c_proj.bias torch.Size([768])\n",
      "h.7.ln_1.weight torch.Size([768])\n",
      "h.7.ln_1.bias torch.Size([768])\n",
      "h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.7.attn.c_attn.bias torch.Size([2304])\n",
      "h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.7.attn.c_proj.bias torch.Size([768])\n",
      "h.7.ln_2.weight torch.Size([768])\n",
      "h.7.ln_2.bias torch.Size([768])\n",
      "h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.7.mlp.c_proj.bias torch.Size([768])\n",
      "h.8.ln_1.weight torch.Size([768])\n",
      "h.8.ln_1.bias torch.Size([768])\n",
      "h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.8.attn.c_attn.bias torch.Size([2304])\n",
      "h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.8.attn.c_proj.bias torch.Size([768])\n",
      "h.8.ln_2.weight torch.Size([768])\n",
      "h.8.ln_2.bias torch.Size([768])\n",
      "h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.8.mlp.c_proj.bias torch.Size([768])\n",
      "h.9.ln_1.weight torch.Size([768])\n",
      "h.9.ln_1.bias torch.Size([768])\n",
      "h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.9.attn.c_attn.bias torch.Size([2304])\n",
      "h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.9.attn.c_proj.bias torch.Size([768])\n",
      "h.9.ln_2.weight torch.Size([768])\n",
      "h.9.ln_2.bias torch.Size([768])\n",
      "h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.9.mlp.c_proj.bias torch.Size([768])\n",
      "h.10.ln_1.weight torch.Size([768])\n",
      "h.10.ln_1.bias torch.Size([768])\n",
      "h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.10.attn.c_attn.bias torch.Size([2304])\n",
      "h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.10.attn.c_proj.bias torch.Size([768])\n",
      "h.10.ln_2.weight torch.Size([768])\n",
      "h.10.ln_2.bias torch.Size([768])\n",
      "h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.10.mlp.c_proj.bias torch.Size([768])\n",
      "h.11.ln_1.weight torch.Size([768])\n",
      "h.11.ln_1.bias torch.Size([768])\n",
      "h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.11.attn.c_attn.bias torch.Size([2304])\n",
      "h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.11.attn.c_proj.bias torch.Size([768])\n",
      "h.11.ln_2.weight torch.Size([768])\n",
      "h.11.ln_2.bias torch.Size([768])\n",
      "h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.11.mlp.c_proj.bias torch.Size([768])\n",
      "ln_f.weight torch.Size([768])\n",
      "ln_f.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Model.from_pretrained(\"/data/models/llm/pytorch/gpt/gpt2\")\n",
    "print(model)\n",
    "state_dict = model.state_dict()\n",
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ee221-970b-4424-b483-2635903febfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
