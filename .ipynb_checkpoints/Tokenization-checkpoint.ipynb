{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2782bc2-7e60-4409-8f87-c3b4009aad58",
   "metadata": {},
   "source": [
    "## 1.Tokenizer\n",
    "### 1.1 Char to INT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c247832e-4a7f-4e21-951b-233cd1577127",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T11:52:02.984723Z",
     "iopub.status.busy": "2024-08-06T11:52:02.984520Z",
     "iopub.status.idle": "2024-08-06T11:52:02.989854Z",
     "shell.execute_reply": "2024-08-06T11:52:02.989644Z",
     "shell.execute_reply.started": "2024-08-06T11:52:02.984705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128079,\n",
       " 127995,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘ğŸ» (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08652d-0b84-4b29-b208-a966118889fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T11:52:02.990414Z",
     "iopub.status.busy": "2024-08-06T11:52:02.990334Z",
     "iopub.status.idle": "2024-08-06T11:52:02.995219Z",
     "shell.execute_reply": "2024-08-06T11:52:02.995018Z",
     "shell.execute_reply.started": "2024-08-06T11:52:02.990406Z"
    }
   },
   "source": [
    "### 1.2 use utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb8b7c9-a302-42ae-981a-f41a2695dae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T11:52:02.995663Z",
     "iopub.status.busy": "2024-08-06T11:52:02.995529Z",
     "iopub.status.idle": "2024-08-06T11:52:02.999629Z",
     "shell.execute_reply": "2024-08-06T11:52:02.999463Z",
     "shell.execute_reply.started": "2024-08-06T11:52:02.995655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236,\n",
       " 149,\n",
       " 136,\n",
       " 235,\n",
       " 133,\n",
       " 149,\n",
       " 237,\n",
       " 149,\n",
       " 152,\n",
       " 236,\n",
       " 132,\n",
       " 184,\n",
       " 236,\n",
       " 154,\n",
       " 148,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 143,\n",
       " 240,\n",
       " 159,\n",
       " 143,\n",
       " 187,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"ì•ˆë…•í•˜ì„¸ìš” ğŸ‘ğŸ» (hello in Korean!)\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c45ad-0119-4fe8-afa2-f4c26110a023",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizer - Simple Convert And Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8fbd3bf-6511-4cc8-af64-fe0d6ae332c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-06T12:08:21.577970Z",
     "iopub.status.busy": "2024-08-06T12:08:21.577704Z",
     "iopub.status.idle": "2024-08-06T12:08:21.583091Z",
     "shell.execute_reply": "2024-08-06T12:08:21.582444Z",
     "shell.execute_reply.started": "2024-08-06T12:08:21.577952Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "text-length =  533\n",
      "-----\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "tokens-length =  616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))\n",
    "print(\"-----\")\n",
    "print(text)\n",
    "print(\"text-length = \", len(text))\n",
    "print(\"-----\")\n",
    "print(tokens)\n",
    "print(\"tokens-length = \", len(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4327ffe-0a3d-4417-b320-591e677381d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T02:35:53.214872Z",
     "iopub.status.busy": "2024-08-08T02:35:53.214595Z",
     "iopub.status.idle": "2024-08-08T02:35:53.223221Z",
     "shell.execute_reply": "2024-08-08T02:35:53.223011Z",
     "shell.execute_reply.started": "2024-08-08T02:35:53.214854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (240, 159) into a new token 257\n",
      "merging (226, 128) into a new token 258\n",
      "merging (105, 110) into a new token 259\n",
      "merging (115, 32) into a new token 260\n",
      "merging (97, 110) into a new token 261\n",
      "merging (116, 104) into a new token 262\n",
      "merging (257, 133) into a new token 263\n",
      "merging (257, 135) into a new token 264\n",
      "merging (97, 114) into a new token 265\n",
      "merging (239, 189) into a new token 266\n",
      "merging (258, 140) into a new token 267\n",
      "merging (267, 264) into a new token 268\n",
      "merging (101, 114) into a new token 269\n",
      "merging (111, 114) into a new token 270\n",
      "merging (116, 32) into a new token 271\n",
      "merging (259, 103) into a new token 272\n",
      "merging (115, 116) into a new token 273\n",
      "merging (261, 100) into a new token 274\n",
      "merging (32, 262) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "# stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "# print(sorted(((v, k)for k, v in stats.items()), reverse=True))\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i< len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    \n",
    "    return newids\n",
    "\n",
    "\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"merging {pair} into a new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfdd84-006c-4c04-ac0a-44313f6faf86",
   "metadata": {},
   "source": [
    "## 2. Encoding\n",
    "### 2.1 decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ef86bf8-b5c0-40fb-bdc4-2ef970847bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T05:56:30.327747Z",
     "iopub.status.busy": "2024-08-08T05:56:30.327404Z",
     "iopub.status.idle": "2024-08-08T05:56:30.332589Z",
     "shell.execute_reply": "2024-08-08T05:56:30.332060Z",
     "shell.execute_reply.started": "2024-08-08T05:56:30.327728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cb501-f913-48ae-a633-e68d3081fd6b",
   "metadata": {},
   "source": [
    "### 2.2 encodeing - the other way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "508dbf04-adb9-4b3e-a10b-84108add4165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T05:56:49.813644Z",
     "iopub.status.busy": "2024-08-08T05:56:49.813233Z",
     "iopub.status.idle": "2024-08-08T05:56:49.818494Z",
     "shell.execute_reply": "2024-08-08T05:56:49.817963Z",
     "shell.execute_reply.started": "2024-08-08T05:56:49.813625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 270, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d516acbb-1024-46a7-8ff9-fa384c478e54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T05:57:21.332731Z",
     "iopub.status.busy": "2024-08-08T05:57:21.332424Z",
     "iopub.status.idle": "2024-08-08T05:57:21.336162Z",
     "shell.execute_reply": "2024-08-08T05:57:21.335827Z",
     "shell.execute_reply.started": "2024-08-08T05:57:21.332713Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f65d3-a863-4882-b723-c592176d24da",
   "metadata": {},
   "source": [
    "### 2.3 Forced splits using regex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b36cc37d-8778-491f-8997-3e7fb9c2b4fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-08T06:41:27.344185Z",
     "iopub.status.busy": "2024-08-08T06:41:27.343901Z",
     "iopub.status.idle": "2024-08-08T06:41:28.078202Z",
     "shell.execute_reply": "2024-08-08T06:41:28.077565Z",
     "shell.execute_reply.started": "2024-08-08T06:41:27.344163Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple/\n",
      "Requirement already satisfied: regex in /data/anaconda3/envs/jupyterlab/lib/python3.10/site-packages (2024.7.24)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "['Hello', ' world', ' how', ' are', ' you']\n"
     ]
    }
   ],
   "source": [
    "%pip install regex\n",
    "\n",
    "import regex as re\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello world how are you\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a5aaa-7fe2-4eab-a5ed-b04332fc52c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
